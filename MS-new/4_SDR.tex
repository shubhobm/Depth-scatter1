\section{Robust PCA and supervised models}
\label{section:sdrSection}
In the presence of a vector of univariate responses, say $\bfy = (y_1,\ldots, y_n)^T$, each $y_i$ being i.i.d. relalizations of a univariate random variable $Y$ that takes values in $\cY \subseteq \BR$, there is substantial literature devoted to utilizing the subspace generated by the basis of $\Sigma = \BV \bfX$ in modelling $\BE(Y|\bfX)$. This ranges from the simple Principal Components Regression (PCR) to Partial Least Squares (PLS) and Envelope methods \citep{Cook10}. In this section, we concentrate on robust inference using Sufficient Dimension Reduction (SDR) \citep{AdragniCook09}, mainly because it provides a general framework for reducing dimensionality of data directly using top eigenvectors of $\Sigma$ (albeit in a different manner than PCR) or an appropriate affine transformation of it.

SDR attempts to find out a linear transformation $\bfR$ on $\bfX$ such that $\BE(Y|\bfX) = \BE(Y| \BR(\bfX))$. Assuming that $\bfR(\bfX)$ takes values in $\mathbb R^d, d \leq \min(n,p)$, this can be achieved through an inverse regression model:
%
\begin{equation}
\bfX_y = \bfmu + \Gamma_d \bfv_y + \bfepsilon,
\end{equation}
%
where $\bfX_y = \bfX|Y=y$, $\Gamma_d \in \BR^{p \times d}$ is a semi-orthogonal basis for $\cS_\Gamma$, the spanning subspace of $\{ \BE \bfX_y - \bfmu | y \in \cY \}$, $\bfv_y = (\Gamma_d^T \Gamma_d)^{-1} \Gamma_d^T (\BE \bfX_y - \bfmu) \in \mathbb R^d$, and $\bfepsilon \sim \cN_p ({\bf 0}, \Delta)$ for some positive-definite matix $\Delta$. This formulation is straightforward to implement when $Y$ is categorical, while for continuous responses, the vector $\bfy$ is divided into a number of slices.

Under this model the minimal sufficient transformation is $\bfR(\bfX) = \Gamma_d^T \Delta^{-1} \bfX$ \citep{AdragniCook09}. The simplest case of this model is when $\Delta = \sigma^2 \BI_p$, for which the maximum likelihood estimator of $ \bfR(\bfX)$ turns out to be the first $d$ PCs of $\Sigma$. Taking $\hat \bfmu_y = \sum_{i=1}^n \bfx_i \BI \{y_i = y\}/n$ and $\hat \bfmu = \sum_{i=1}^n \bfx_i /n$, one can now estimate $\sigma^2$ as: $\hat\sigma^2 = \sum_{i=1}^p s_i /p$, where $s_i$ is the $i^\text{th}$ diagonal element of $\BC_y^T \BC_y/n$, with
%
$$
\BC_y = (\bfc_{y1}, \ldots, \bfc_{yn_y})^T; \quad
n_y = \sum_{i=1}^n \BI \{ y_i = y \},
\bfc_{yi} = \bfx_{yi} - \hat \bfmu - \hat\Gamma_d (\hat \Gamma_d^T \hat \Gamma_d)^{-1} \hat \Gamma_d^T (\hat \bfmu_y - \hat \bfmu ).
$$
%
Following this, predictions for a new observation $\bfx$ is obtained as a weighted sum of the responses:
%
$$
\hat \BE(Y|\bfX=\bfx) = \frac{\sum_{i=1}^n w_i y_i}{\sum_{i=1}^n w_i}; \quad w_i = \exp \left[ -\frac{1}{\hat\sigma^2}  \| \hat\Gamma_d^T (\bfx - \bfx_i) \|^2 \right].
$$
%

We formulate a robust version of the above procedure by estimating the quantities $\Gamma_d, \bfmu, \bfmu_y, \sigma^2$ by robust methods. Specifically, we take:
%
\begin{itemize}
\item $\tilde \Gamma_d = $ first $d$ eigenvectors of the sample DCM;
%
\item $\tilde \bfmu = $ weighted spatial median of the rows of $\BX$;
%
\item $\tilde \bfmu_y = $ weighted spatial median of the rows of $\BX_y$, for each $y \in \cY$;
%
\item $\tilde\sigma^2 = \sum_{i=1}^p \tilde s_i/p$, where $\tilde s_i$ is the $i^\text{th}$ diagonal element of $\tilde \BC_y^T \tilde \BC_y/n$, $\tilde \BC_y$ being the matrix composed of rows $\bfx_{yi} - \tilde \bfmu - \tilde \Gamma_d (\tilde \Gamma_d^T \tilde \Gamma_d)^{-1} \tilde \Gamma_d^T (\tilde \bfmu_y - \tilde \bfmu )$ for $i \in \{ 1, \ldots, n \}$ such that $y_i = y$.
\end{itemize}
%
We compare the performance of our robust SDR with the original method of \cite{AdragniCook09} with or without the presence of bad leverage points in $\Sigma$ using a simulation study. For a given value of $p$, we take $n=200, d=1$, generate the responses $y_1, \ldots, y_n$ as independent standard normal, and the predictors as $\bfX_y = {\bf 1} (y + y^2 + y^3) + \bfepsilon$, with ${\bf 1} \in \BR^p$ consisting of all 1's, and $\BV(\bfepsilon) = 25 \BI_p$. We measure performances of the SDR models by their mean squared prediction error on another set of 200 observations $(\bfy^*, \BX^*)$ generated similarly, and taking the average of these errors on 100 such training-test pairs of datasets. Finally we repeat the full setup for different choices of $p = 5,10,25,50,75,100,125,150$.

\begin{figure}[t]
%\captionsetup{justification=centering, font=footnotesize}
\begin{center}
\subfigure[]{\epsfxsize=0.35\linewidth \epsfbox{../Codes/SDRcomparison_noout}}
\subfigure[]{\epsfxsize=0.35\linewidth \epsfbox{../Codes/SDRcomparison_out}}
\caption{Average prediction errors for two methods of SDR (a) in absence and (b) in presence of outliers}
\label{fig:SDRfig}
\end{center}
\end{figure}

Panel (a) of figure \ref{fig:SDRfig} compares prediction errors using robust and maximum likelihood SDR estimates when $\BX$ contains no outliers: here the two methods are virtually indistinguishable. We now introduce outliers in each of the 100 datasets by adding 100 to first $p/5$ coordinates of the first 10 observations in $\BX$, and repeat the analysis. Panel (b) of the figure shows that the robust SDR method remains more accurate in predicting out of sample observations for all values of $p$ than standard SDR.

