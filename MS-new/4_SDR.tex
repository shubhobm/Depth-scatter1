\section{Robust PCA and supervised models}

In the presence of a vector of univariate responses, say $\bfY = (Y_1,Y_2,...,Y_n)^T$, there is substantial literature devoted to utilizing the subspace generated by the basis of $Cov(\bfX)$ in modelling $E(Y|\bfX)$. This ranges from the simple Principal Components Regression (PCR) to Partial Least Squares (PLS) and Envelope methods \citep{Cook10}. Here we concentrate on robust inference using Sufficient Dimension Reduction (SDR) \citep{AdragniCook09}, mainly because it provides a general framework for reducing dimensionality of data directly using top eigenvectors of the covariance matrix of $X$ (albeit in a different manner than PCR) or an appropriate affine transformation of it.

SDR attempts to find out a linear transformation $R$ on $\bfX$ such that $E(Y|\bfX) = E(Y|R(\bfX))$. Assuming that $R(\bfX)$ takes values in $\mathbb R^d, d \leq \min(n,p)$, this can be achieved through an inverse regression model:
%
\begin{equation}
\bfX_y = \bar \bfmu + \Gamma \bfv_y + \bfepsilon
\end{equation}
%
where $\bfX_y = \bfX|Y=y, \bar\bfmu = E\bfX$, $\Gamma$ is a $p \times d$ semi-orthogonal basis for $\mathcal S_\Gamma$, the spanning subspace of $\{ E \bfX_y - \bar\bfmu | y \in S _Y \}$ ($S_y$ is sample space of $Y$) and $\bfv_y = (\Gamma^T \Gamma)^{-1} \Gamma^T (E \bfX_y - \bar\bfmu) \in \mathbb R^d$. The random error term $\bfepsilon$ follows a multivariate normal distribution with mean ${\bf 0}_p$ and covariance matrix $\Delta$. This formulation is straightforward to implement when $Y$ is categorical, while for continuous responses, the vector $\bfy$ is divided into a number of slices.

Under this model the minimal sufficient transformation is $R(\bfX) = \Gamma^T \Delta^{-1} \bfX$. The simplest case of this model is when $\Delta = \sigma^2 I_p$, for which the maximum likelihood estimator of $ R(\bfX)$ turns out to be the first $d$ PCs of $Cov(\bfX)$. Taking $\hat E\bfX_y = \bar\bfX_y$ and $\hat{\bar\bfmu} = \bar\bfX$, one can now estimate $\sigma^2$ as: $\hat\sigma^2 = \sum_{i=1}^p s_{ii}/p$, where $s_{ii}$ is the $i^\text{th}$ diagonal element of $\hat{Cov}_Y( \bfX_Y - \bar\bfX - \hat\Gamma \hat\bfv_Y)$. Following this, predictions for a new observation $\bfx$ is obtained as a weighted sum of the responses:
%
$$
\hat E(Y|\bfX=\bfx) = \frac{\sum_{i=1}^n w_i Y_i}{\sum_{i=1}^n w_i}; \quad w_i = \exp \left[ -\frac{1}{\hat\sigma^2}  \| \hat\Gamma^T (\bfx - \bfX_i) \|^2 \right]
$$
%

We formulate a robust version of the above procedure by estimating the quantities $\Gamma, \bar\bfmu, \bfmu_y, \sigma^2$ by robust methods. Specifically, we take:
%
\begin{itemize}
\item $\tilde \Gamma = $ first $d$ eigenvectors of the sample DCM;
%
\item $\tilde{\bar\bfmu} = $ spatial median of the rows of $X$;
%
\item $\tilde \bfmu_y = $ spatial median of the rows of $(X|Y=y)$, for all $y \in S_Y$;
%
\item $\tilde\sigma^2 = \sum_{i=1}^p [\widehat{\text{MAD}}_Y (X_{Y,i} - \tilde{\bar\mu}_i - \tilde\bfgamma_i^T \tilde\bfv_Y)]^2/p$, with $\tilde\Gamma = (\tilde\bfgamma_1, ..., \tilde\bfgamma_p)^T$.
\end{itemize}
%
The following simulation study using the same setup as in \citep{AdragniCook09} compares the performance of our robust SDR with the original method with or without the presence of bad leverage points in the covariate matrix $X$. For a fixed dimension $p$, we take $n=200, d=1$, generate the responses $Y$ as independent standard normal, and the predictors as $\bfX_Y = \bfgamma^* v_Y^* + \bfepsilon$, with $\bfgamma^*_{p\times 1} = (1,...,1)^T, v_Y = Y + Y^2 + Y^3$ and $Var(\bfepsilon) = 25 I_p$. We measure performance of both SDR models by their mean squared prediction error on another set of 200 observations $(Y^*, \bfX^*)$ generated similarly, and taking the average of these errors on 100 such training-test pair of datasets. Finally we repeat the whole setup for different choices of $p = 5,10,25,50,75,100,125,150$.

\begin{figure}[t]
%\captionsetup{justification=centering, font=footnotesize}
\begin{center}
\subfigure[]{\epsfxsize=0.35\linewidth \epsfbox{../Codes/SDRcomparison_noout}}
\subfigure[]{\epsfxsize=0.35\linewidth \epsfbox{../Codes/SDRcomparison_out}}
\caption{Average prediction errors for two methods of SDR (a) in absence and (b) in presence of outliers}
\label{fig:SDRfig}
\end{center}
\end{figure}

Panel (a) of figure \ref{fig:SDRfig} compares prediction errors using robust and maximum likelihood SDR estimates when $X$ contains no outliers, and the two methods are virtually indistinguishable. We now introduce outliers in each of the 100 datasets by adding 100 to first $p/5$ coordinates of the first 10 observations in $X$, and repeat the analysis. Panel (b) of the figure shows that although our robust method performs slightly worse than the case when there were no outliers, it remains more accurate in predicting our of sample observations for all values of $p$. 

