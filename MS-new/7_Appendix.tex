
\appendix
\section*{Appendix}
\numberwithin{equation}{section}
\section{\textbf{Form of $V_{D,S}(F)$}}\label{section:appA}

First observe that for $F$ having covariance matrix $\Sigma = \Gamma\Lambda\Gamma^T$,
%
$$ V_{D,S}(F)  = (\Gamma \otimes \Gamma) V_{D,S}(F_\Lambda) (\Gamma \otimes \Gamma)^T$$
%
where $F_\Lambda$ has the same elliptic distribution as $F$, but with covariance matrix $\Lambda$. Now,
%
\begin{eqnarray*}
V_{D,S} (F_\Lambda) &=& E \left[ vec \left\{ \frac{(\tilde D_\bfZ (\bfz))^2 \Lambda^{1/2} \bfz\bfz^T \Lambda^{1/2}}{\bfz^T\Lambda\bfz} - \Lambda_{D,S} \right\} vec^T \left\{ \frac{(\tilde D_\bfZ (\bfz))^2 \Lambda^{1/2} \bfz\bfz^T \Lambda^{1/2}}{\bfz^T\Lambda\bfz} - \Lambda_{D,S} \right\} \right]\\
&=& E \left[ vec \left\{ (\tilde D_\bfZ (\bfz))^2 SS(\Lambda^{1/2}\bfz; \bf0) \right\} vec^T \left\{ (\tilde D_\bfZ (\bfz))^2 SS(\Lambda^{1/2}\bfz; \bf0) \right\} \right]\\
&& - \quad vec(\Lambda_{D,S}) vec^T(\Lambda_{D,S})
\end{eqnarray*}

The matrix $vec(\Lambda_{D,S}) vec^T(\Lambda_{D,S})$ consists of elements $\lambda_i\lambda_j$ at $(i,j)^\text{th}$ position of the $(i,j)^\text{th}$ block, and 0 otherwise. These positions correspond to variance and covariance components of on-diagonal elements. For the expectation matrix, all its elements are of the form $E[\sqrt{\lambda_a \lambda_b \lambda_c \lambda_d} z_a z_b z_c z_d . (\tilde D_\bfZ (\bfz))^4 / (\bfz^T \Lambda \bfz)^2]$, with $1 \leq a,b,c,d \leq p$. Since $(\tilde D_\bfZ (\bfz))^4 / (\bfz^T \Lambda \bfz)^2$ is even in $\bfz$, which has a circularly symmetric distribution, all such expectations will be 0 unless $a=b=c=d$, or they are pairwise equal. Following a similar derivation for spatial sign covariance matrices in \cite{magyar14}, we collect the non-zero elements and write the matrix of expectations:
%
$$ (I_{p^2} + K_{p,p}) \left\{ \sum_{a=1}^p \sum_{b=1}^p \gamma^D_{ab} (\bfe_a \bfe_a^T \otimes  \bfe_b \bfe_b^T) - \sum_{a=1}^p \gamma^D_{aa} (\bfe_a \bfe_a^T \otimes  \bfe_a \bfe_a^T) \right\} + \sum_{a=1}^p \sum_{b=1}^p \gamma^D_{ab} (\bfe_a \bfe_b^T \otimes  \bfe_a \bfe_b^T) $$
%
where $I_k = (\bfe_1,...,\bfe_k), K_{m,n} = \sum_{i=1}^m \sum_{j=1}^n J_{ij} \otimes J_{ij}^T$ with $J_{ij}$ the $m \times n$ matrix having 1 as $(i,j)^\text{th}$ element and 0 elsewhere, and $\gamma^D_{mn} = E[ \lambda_m \lambda_n z_m^2 z_n ^2 . (\tilde D_\bfZ (\bfz))^4 / (\bfz^T \Lambda \bfz)^2]; 1 \leq m,n \leq p$.

\paragraph{}Putting everything together, denote $\hat S^D(F_\Lambda) = \sum_{i=1}^n (\tilde D^n_\bfZ (\bfz_i))^2 SS(\Lambda^{1/2}\bfz_i; \hat \bfmu_n)/n $. Then the different types of elements in the matrix $V_{D,S}(F_\Lambda)$ are as given below ($1 \leq a,b,c,d \leq p$):

\begin{itemize}
\item Variance of on-diagonal elements
%
$$ AVar( \sqrt n \hat S^D_{aa} (F_\Lambda)) = E \left[ \frac{(\tilde D_\bfZ (\bfz))^4 \lambda_a^2 z_a^4}{(\bfz^T \Lambda \bfz)^2} \right] - \lambda_{D,S,a}^2 $$

\item Variance of off-diagonal elements ($a \neq b$)
%
$$ AVar( \sqrt n \hat S^D_{ab} (F_\Lambda)) = E \left[ \frac{(\tilde D_\bfZ (\bfz))^4 \lambda_a \lambda_b z_a^2 z_b^2}{(\bfz^T \Lambda \bfz)^2} \right] $$

\item Covariance of two on-diagonal elements ($a \neq b$)
%
$$ ACov(\sqrt n \hat S^D_{aa} (F_\Lambda), \sqrt n \hat S^D_{bb} (F_\Lambda))
= E \left[ \frac{(\tilde D_\bfZ (\bfz))^4 \lambda_a \lambda_b z_a^2 z_b^2}{(\bfz^T \Lambda \bfz)^2} \right] - \lambda_{D,S,a} \lambda_{D,S,b} $$

\item Covariance of two off-diagonal elements ($a \neq b \neq c \neq d$)
%
$$ ACov(\sqrt n \hat S^D_{ab} (F_\Lambda), \sqrt n \hat S^D_{cd} (F_\Lambda)) = 0 $$

\item Covariance of one off-diagonal and one on-diagonal element ($a \neq b \neq c$)
%
$$ ACov(\sqrt n \hat S^D_{ab} (F_\Lambda), \sqrt n \hat S^D_{cc} (F_\Lambda)) = 0 $$
\end{itemize}

\section{Asymptotics of eigenvectors and eigenvalues}\label{section:appB}
The following result allows us to obtain asymptotic joint distributions of eigenvectors and eigenvalues of the sample DCM, provided we know the limiting distribution of the sample DCM itself:

\begin{Theorem} \label{Theorem:decomp} \citep{taskinen12}
Let $F_\Lambda$ be an elliptical distribution with a diagonal covariance matrix $\Lambda$, and $\hat C$ be any positive definite symmetric $p \times p$ matrix such that at $F_\Lambda$ the limiting distribution of $\sqrt n vec(\hat C - \Lambda)$ is a $p^2$-variate (singular) normal distribution with mean zero. Write the spectral decomposition of $\hat C$ as $\hat C = \hat P \hat\Lambda \hat P^T$. Then the limiting distributions of $\sqrt n vec(\hat P - I_p)$ and $\sqrt n vec(\hat\Lambda - \Lambda)$ are multivariate (singular) normal and
%
\begin{equation} \label{equation:decompEq}
\sqrt n vec (\hat C - \Lambda)  = \left[ (\Lambda \otimes I_p) - (I_p \otimes \Lambda) \right] \sqrt n vec (\hat P - I_p) + \sqrt n vec (\hat\Lambda - \Lambda) + o_P(1)
\end{equation}
\end{Theorem}

The first matrix picks only off-diagonal elements of the LHS and the second one only diagonal elements. We shall now use this as well as the form of the asymptotic covariance matrix of the vec of sample DCM, i.e. $V_{D,S}(F)$ to obtain limiting variance and covariances of eigenvalues and eigenvectors.

\begin{Corollary} \label{Corollary:eigendist}
Consider the sample DCM $ \hat S^D(F) = \sum_{i=1}^n (\tilde D^n_\bfX (\bfx_i))^2 SS(\bfx_i; {\bf \hat\bfmu_n})/n $ and its spectral decomposition $\hat S^D(F) = \hat\Gamma_D \hat\Lambda_D \hat\Gamma_D^T $. Then the matrices $G = \sqrt n (\hat\Gamma_D - \Gamma) $ and $L = \sqrt n (\hat\Lambda_D - \Lambda_{D,S}) $ have independent distributions. The random variable $vec(G)$ asymptotically has a $p^2$-variate normal distribution with mean ${\bf 0}_{p^2}$, and the asymptotic variance and covariance of different columns of $G = (\bfg_1,...,\bfg_p)$ are as follows:
%
\begin{equation} \label{equation:DevEq}
AVar(\bfg_i) = \sum_{k=1; k \neq i}^p \frac{1}{(\lambda_{D,s,k} - \lambda_{D,S,i})^2} E \left[ \frac{(\tilde D_\bfZ (\bfz))^4 \lambda_i \lambda_k z_i^2 z_k^2}{(\bfz^T \Lambda \bfz)^2} \right] \bfgamma_k \bfgamma_k^T
\end{equation}
%
\begin{equation}
ACov(\bfg_i, \bfg_j) = - \frac{1}{(\lambda_{D,s,i} - \lambda_{D,S,j})^2} E \left[ \frac{(\tilde D_\bfZ (\bfz))^4 \lambda_i \lambda_j z_i^2 z_j^2}{(\bfz^T \Lambda \bfz)^2} \right] \bfgamma_j \bfgamma_i^T; \quad i \neq j
\end{equation}
%
where $\Gamma = (\bfgamma_1,...,\bfgamma_p)$. The vector consisting of diagonal elements of $L$, say $\bfl = (\l_1,...,\l_p)^T$ asymptotically has a $p$-variate normal distribution with mean ${\bf 0}_p$ and variance-covariance elements:
%
\begin{eqnarray}
AVar(l_i) &=& E \left[ \frac{(\tilde D_\bfZ (\bfz))^4 \lambda_i^2 z_i^4}{(\bfz^T \Lambda \bfz)^2} \right] - \lambda_{D,S,i}^2\\
ACov(l_i, l_j) &=& E \left[ \frac{(\tilde D_\bfZ (\bfz))^4 \lambda_i \lambda_j z_i^2 z_j^2}{(\bfz^T \Lambda \bfz)^2} \right] - \lambda_{D,S,i} \lambda_{D,S,j}; \quad i \neq j
\end{eqnarray}
%
\end{Corollary}

%\begin{proof}[Proof of Theorem \ref{Theorem:decomp}]
%See \cite{taskinen12}
%\end{proof}

\begin{proof}[Proof of Corollary \ref{Corollary:eigendist}]
In spirit, this corollary is similar to Theorem 13.5.1 in \cite{anderson}. Due to the decomposition (\ref{equation:decompEq}) we have, for the distribution $F_\Lambda$, the following relation between any off-diagonal element of $\hat S^D(F_\Lambda)$ and the corresponding element in the estimate of eigenvectors $\hat\Gamma_D (F_\Lambda)$:

$$ \sqrt n \hat\gamma_{D,ij} (F_\Lambda) = \sqrt n \frac{\hat S^D_{ij} (F_\Lambda)}{\lambda_{D,S,i} - \lambda_{D,S,j}}; \quad i \neq j$$

So that for eigenvector estimates of the original $F$ we have

\begin{equation} \label{equation:app1}
\sqrt n (\hat\bfgamma_{D,i} - \bfgamma_i) = \sqrt n \Gamma (\hat \bfgamma_{D,i}(F_\Lambda) - \bfe_i ) = \sqrt n \left[ \sum_{k=1; k \neq i}^p \hat \gamma_{D,ik}(F_\Lambda)\bfgamma_k + (\hat \gamma_{D,ii}(F_\Lambda) - 1)\bfgamma_i \right]
\end{equation}

$\sqrt n (\hat \gamma_{D,ii}(F_\Lambda) - 1) =  o_P(1)$ and $ACov(\sqrt n \hat S^D_{ik}(F_\Lambda), \sqrt n \hat S^D_{il}(F_\Lambda)) = 0$ for $k \neq l$, so the above equation implies

$$ AVar(\bfg_i) = AVar (\sqrt n (\hat\bfgamma_{D,i} - \bfgamma_i)) = \sum_{k=1; k \neq i}^p \frac{AVar(\sqrt n \hat S^D_{ik}(F_\Lambda))}{(\lambda_{D,s,i} - \lambda_{D,S,k})^2} \bfgamma_k \bfgamma_k^T $$

For the covariance terms, from (\ref{equation:app1}) we get, for $i \neq j$,

\begin{eqnarray*}
ACov(\bfg_i, \bfg_j) &=& ACov (\sqrt n (\hat\bfgamma_{D,i} - \bfgamma_i), \sqrt n (\hat\bfgamma_{D,j} - \bfgamma_j))\\
&=& ACov \left( \sum_{k=1; k \neq i}^p \sqrt n \hat \gamma_{D,ik}(F_\Lambda)\bfgamma_k, \sum_{k=1; k \neq j}^p \sqrt n \hat \gamma_{D,jk}(F_\Lambda)\bfgamma_k \right)\\
&=& ACov \left( \sqrt n \hat \gamma_{D,ij}(F_\Lambda)\bfgamma_j, \sqrt n \hat \gamma_{D,ji}(F_\Lambda)\bfgamma_i \right)\\
&=& - \frac{AVar(\sqrt n \hat S^D_{ij}(\Lambda))}{(\lambda_{D,s,i} - \lambda_{D,S,j})^2} \bfgamma_j \bfgamma_i^T
\end{eqnarray*}

The exact forms given in the statement of the corollary now follows from the  Form of $V_{D,S}$ in Appendix \ref{section:appA}.

\paragraph{}For the on-diagonal elements of $\hat S^D(F_\Lambda)$ Theorem \ref{Theorem:decomp} gives us $ \sqrt n \hat\lambda_{D,s,i} (F_\Lambda) = \sqrt n \hat S^D_{ii}(F_\Lambda)$ for $i = 1,...,p$. Hence

\begin{eqnarray*}
AVar(l_i) &=& AVar(\sqrt n \hat\lambda_{D,s,i} - \sqrt n \lambda_{D,S,i})\\
&=& AVar(\sqrt n \hat\lambda_{D,s,i} (F_\Lambda) - \sqrt n \lambda_{D,S,i}(F_\Lambda))\\
&=& AVar(\sqrt n S^D_{ii}(F_\Lambda))
\end{eqnarray*}

A similar derivation gives the expression for $AVar(l_i,l_j); i \neq j$. Finally, since the asymptotic covariance between an on-diagonal and an off-diagonal element of $\hat S^D(F_\Lambda)$, it follows that the elements of $G$ and diagonal elements of $L$ are independent.
\end{proof}

\section{Proofs}\label{section:appC}

\begin{proof}[Proof of Proposition \ref{proposition:SignTest}]
Under contiguous alternatives $H_0: \bfmu = \bfmu_0$, the weighted sign test statistic $T_{n,w}$ has mean $E (w(\bfZ) \bfS (\bfZ))$. For spherically symmetric $\bfZ$, $w(\bfZ)$ depends on $\bfZ$ only through its norm. Since $\| \bfZ \|$ and $\bfS(\bfZ)$ are independent, we get $E (w(\bfZ) \bfS (\bfZ)) = E w(\bfZ). E \bfS (\bfZ)$. The same kind of decomposition holds for $Cov(w(\bfZ) \bfS(\bfZ))$.

We can now simplify the approximate local power $\beta_{n,w}$ of the level-$\alpha$ ($0 < \alpha < 1$) test based on $T_{n,w}$:
%
\begin{eqnarray*}
\beta_{n,w} &=& K_p \left( \chi^2_{p,\alpha} + n (E (w(\bfZ) \bfS (\bfZ))^T \right.\\
&& \left. [ E (w^2(\bfZ) \bfS (\bfZ) \bfS (\bfZ)^T) ]^{-1} (E (w(\bfZ) \bfS (\bfZ)) \right)\\
&=& K_p \left( \chi^2_{p,\alpha} + \frac{E^2 w(\bfZ)}{Ew^2(\bfZ)}. E\bfS(\bfZ)^T [Cov(\bfS(\bfZ)]^{-1} E\bfS(\bfZ) \right)
\end{eqnarray*}
%
where $K_p$ and $\chi^2_{p,\alpha}$ are distribution function and upper-$\alpha$ cutoff of a $\chi^2_p$ distribution, respectively. Since $E^2 w(\bfZ) \leq Ew(\bfZ)$, $\beta_{n,w}$ the largest possible value of $\beta_{n,w}$ is $K_p ( \chi^2_{p,\alpha} + E\bfS(\bfZ)^T [Cov(\bfS(\bfZ)]^{-1} E\bfS(\bfZ) )$, the approximate power of the unweighted sign test statistic. Equality is of course achieved when $w(\bfZ)$ is a constant independent of $\bfZ$.
\end{proof}

\begin{proof}[Sketch of proofs for equations \ref{eqn:hdtest1} and \ref{eqn:hdtest2}]

A first step to obtain asymptotic normality for the high-dimensional location test statistic $C_{n,w}$ is obtaining an equivalent result of Lemma 2.1 in \cite{WangPengLi15}:

\begin{Lemma}\label{Lemma:HDlemma21} Under the conditions

\noindent\textbf{(C1)}$\text{Tr}(\Sigma^4) = o(\text{Tr}^2(\Sigma^2)) $,

\noindent\textbf{(C2)}$\text{Tr}^4(\Sigma) / \text{Tr}^2(\Sigma^2) \exp[ - \text{Tr}^2(\Sigma) / 128p \lambda^2_{\max}(\Sigma) ] = o(1)$
\vspace{1em}

\noindent when $H_0$ is true we have
%
\begin{eqnarray}
E[ (\bfepsilon_{w1}^T \bfepsilon_{w2})^4 ] &=& O(1) E^2[ (\bfepsilon_{w1}^T \bfepsilon_{w2})^2 ]\\
E[ (\bfepsilon_{w1}^T B_w \bfepsilon_{w1})^2 ] &=& O(1) E^2[ (\bfepsilon_{w1}^T B_w \bfepsilon_{w1})^2 ]\\
E[ (\bfepsilon_{w1}^T B_w \bfepsilon_{w2})^2 ] &=& o(1) E^2[ (\bfepsilon_{w1}^T B_w \bfepsilon_{w1})^2 ]
\end{eqnarray}
%
with $\bfepsilon \sim \mathcal E({\bf 0}_p, \Lambda, G)$ and $\bfepsilon_w = w(\bfepsilon) \bfS(\bfepsilon)$.
\end{Lemma}
%
A proof of this lemma is derived using results in section 3 of \cite{ElKaroui09}, noticing that any-scalar valued 1-Lipschitz function of $\bfepsilon_w$ is a $M_w$-Lipschitz function of $\bfS(\bfepsilon)$, with $M_w = \sup_\bfepsilon w(\bfepsilon)$. Same steps as in the proof of Theorem 2.2 in \cite{WangPengLi15} follow now, using the lemma above in place of Lemma 2.1 therein, to establish asymptotic normality of $C_{n,w}$ under $H_0$.

To derive the asymptotic distribution under contiguous alternatives we need the conditions (C3)-(C6) in \cite{WangPengLi15}, as well as slightly modified versions of Lemmas A.4 and A.5:

\begin{Lemma}
Given that condition (C3) holds, we have $\lambda_{\max} (B_w) \leq 2 \frac{\lambda_{\max}}{\text{Tr} (\Sigma)} (1+o(1))$.
\end{Lemma}

\begin{Lemma}
Define $D_w = E \left[ \frac{w^2(\bfepsilon)}{\| \bfepsilon \|^2} (I_p - \bfS(\bfepsilon) \bfS(\bfepsilon)^T )\right] $. Then $\lambda_{\max} (A_w) \leq E( w(\bfepsilon)/\| \bfepsilon \|)$ and $\lambda_{\max} (D_w) \leq E( w(\bfepsilon)/\| \bfepsilon \|)^2$. Further, if (C3) and (C4) hold then $\lambda_{\min} (A_w) \geq E( w(\bfepsilon)/\| \bfepsilon \|)(1+o(1))/\sqrt 3$.
\end{Lemma}
%
The proof now exactly follows steps in the proof of theorem 2.3 in \cite{WangPengLi15}, replacing vector signs by weighted signs, using the fact that $w(\bfepsilon)$ is bounded above by $M_w$ while applying conditions (C5)-(C6) and lemmas A.1, A.2, A.3, and finally using the above two lemmas in place of lemmas A.4 and A.5 respectively.
\end{proof}

\begin{proof}[Proof of Theorem  \ref{Theorem:covform}]
The proof follows directly from writing out the expression of $Cov ( \tilde \bfX)$:
%
\begin{eqnarray*}
Cov(\tilde\bfX) &=& E(\tilde\bfX \tilde\bfX^T) - E(\tilde\bfX) E(\tilde\bfX)^T\\
&=& \Gamma . E \left[ (\tilde D_\bfZ(\bfz))^2 \frac{\|\bfz\|^2}{\| \Lambda^{1/2}\bfz\|} \Lambda^{1/2} \bfS(\bfz) \bfS(\bfz)^T \Lambda^{1/2} \right] \Gamma^T - {\bf 0}_p {\bf 0}_p^T\\
&=& \Gamma .E \left[ (\tilde D_\bfZ(\bfz))^2 \frac{\Lambda^{1/2} \bfz \bfz^T \Lambda^{1/2}}{\bfz^T \Lambda \bfz} \right] \Gamma^T
\end{eqnarray*}
%
\end{proof}

\begin{proof}[Proof of Lemma \ref{Lemma:lemma1}]
For two positive definite matrices $A,B$, we denote by $A>B$ that $A-B$ is positive definite. Also, denote
%
$$ S_n = \sqrt n \left[ \frac{1}{n} \sum_{i=1}^n \left| (\tilde D^{n} _\bfX (\bfx_i))^2  - (\tilde D_\bfX (\bfx_i))^2 \right| SS(\bfx_i; \hat\bfmu_n) \right] $$
%
%Consider now the quantity $\bft^T S_n \bft$, where $\bft \in \mathbb{R}^p$. Now applying Cauchy-Schwarz inequality we shall have
%%
%\begin{eqnarray}\label{eqn:AppEqn1}
%\frac{(\bft^T S_n \bft)^2}{n} & = & \left[ \frac{1}{n} \sum_{i=1}^n \left| (\tilde D^{n} _\bfX (\bfx_i))^2  - (\tilde D_\bfX (\bfx_i))^2 \right|^2  \bft^T SS(\bfx_i; \hat\bfmu_n) \bft \right]^2 \notag\\
%& \leq & \left[ \frac{1}{n} \sum_{i=1}^n \left| (\tilde D^{n} _\bfX (\bfx_i))^2  - (\tilde D_\bfX (\bfx_i))^2 \right|^2 \right] \left[ \frac{1}{n} \left( \sqrt n.\frac{1}{n} \sum_{i=1}^n  SS(\bfx, \hat\bfmu_n) \right) \right]
%\end{eqnarray}
%%
%with $\hat S_n = \sum_{i=1}^n SS(\bfx_i, \hat\bfmu_n)/ \sqrt n$.
Now due to the assumption of uniform convergence, given $\epsilon>0$ we can find $N \in \mathbb{N}$ such that
%
\begin{equation}
\label{equation:lemma1eq}
| (\tilde D^{n_1}_\bfX(\bfx_i))^2 - (\tilde D_\bfX(\bfx_i))^2 | < \epsilon
\end{equation}
%
for all $n_1 \geq N; i = 1,2,...,n_1$. This implies
%
\begin{eqnarray}
\label{eqn:lemma1eq2}
S_{n_1} &< & \epsilon \sqrt{n_1} \left[ \frac{1}{n_1} \sum_{i=1}^{n_1} SS(\bfx_i; \hat\bfmu_{n_1}) \right]\notag\\
&=& \epsilon \sqrt{n_1} \left[ \frac{1}{n_1} \sum_{i=1}^{n_1} \left\{ SS(\bfx_i; \hat\bfmu_{n_1}) - SS(\bfx_i; \bfmu) \right\} + \frac{1}{n_1} \sum_{i=1}^{n_1} SS(\bfx_i; \bfmu) \right]
\end{eqnarray}

We now construct a sequence of positive definite matrices $\{A_k (B_k+C_k) : k \in \mathbb N\} $ so that
%
$$ A_k = \frac{1}{k}, \quad B_k = \sqrt{N_k} \left[ \frac{1}{N_k} \sum_{i=1}^{N_k} \left\{ SS(\bfx_i; \hat\bfmu_{N_k}) - SS(\bfx_i; \bfmu) \right\} \right]$$
$$\quad C_k = \sqrt{N_k} \left[ \frac{1}{N_k} \sum_{i=1}^{N_k} SS(\bfx_i; \bfmu) \right] $$
%
where $N_k \in \mathbb N$ gives the relation (\ref{equation:lemma1eq}) in place of $N$ when we take $\epsilon = 1/k$. Under conditions $ E\|\bfx - \bfmu\|^{-3/2} < \infty $ and $\sqrt n (\hat\bfmu_n - \bfmu) = O_P(1)$, the sample SCM with unknown location parameter $\hat\bfmu_n$ has the same asymptotic distribution as the SCM with known location $\bfmu$ \citep{durre14}, hence $B_k = o_P(1)$, thus $A_k (B_k+C_k) \stackrel{P}{\rightarrow} 0$.

Now (\ref{eqn:lemma1eq2}) implies that for any $\epsilon_1 > 0$, $S_{N_k} > \epsilon_1 \Rightarrow A_k (B_k + C_k) > \epsilon_1$, which means $ P(S_{N_k} > \epsilon_1) < P(A_k (B_k + C_k) > \epsilon_1)$. Hence the subsequence $\{S_{N_k}\} \stackrel{P}{\rightarrow} 0$. Since the main sequence $\{S_k\}$ is bounded below by 0, this implies $\{S_k\} \stackrel{P}{\rightarrow} 0$. Finally, we have that
%
\begin{eqnarray}
\sqrt n \left[
\frac{1}{n} \sum_{i=1}^n (\tilde D^n_\bfX (\bfx_i))^2 SS(\bfx_i; \hat\bfmu_n) -
\frac{1}{n} \sum_{i=1}^n (\tilde D_\bfX (\bfx_i))^2 SS(\bfx_i; \bfmu) \right] &\leq & \hspace{1em} \notag\\
S_n +  \sqrt{n} \left[ \frac{1}{n} \sum_{i=1}^{n} \left\{ SS(\bfx_i; \hat\bfmu_{n}) - SS(\bfx_i; \bfmu) \right\} \right] &&
\end{eqnarray}
%
Since the second summand on the right hand side is $o_P(1)$ due to \cite{durre14} as mentioned before, we have the needed.
\end{proof}

\begin{proof}[Proof of Theorem \ref{Theorem:rootn}]
The quantity in the statement of the theorem can be broken down as:
%
\begin{eqnarray*}
\sqrt n \left[ vec\left\{ \frac{1}{n} \sum_{i=1}^n (\tilde D^n_\bfX (\bfx_i))^2 SS(\bfx_i; \hat\bfmu_n) \right\} - vec\left\{ \frac{1}{n} \sum_{i=1}^n (\tilde D_\bfX (\bfx_i))^2 SS(\bfx_i; \bfmu) \right\} \right] +\\
\sqrt n \left[ vec\left\{ \frac{1}{n} \sum_{i=1}^n (\tilde D_\bfX (\bfx_i))^2 SS(\bfx_i; \bfmu) \right\} - E \left[ vec\left\{ (\tilde D_\bfX (\bfx))^2 SS(\bfx; \bfmu) \right\} \right] \right]
\end{eqnarray*}
%
The first part goes to 0 in probability by Lemma \ref{Lemma:lemma1}, and applying Slutsky's theorem we get the required convergence.
\end{proof}

\begin{proof}[Proof of Theorem \ref{Thm:pluginSigma}]
We are going to prove the following:
%
\begin{enumerate}
\item $\| \hat\Gamma_D - \Gamma \|_F \stackrel{P}{\rightarrow} 0$, and

\item $\| \hat\Lambda - \Lambda \|_F \stackrel{P}{\rightarrow} 0$
\end{enumerate}
%
as $n \rightarrow \infty$. For (1), we notice $\sqrt n vec(\hat\Gamma_D - \Gamma)$ asymptotically has a (singular) multivariate normal distribution following Corollary \ref{Corollary:eigendist}, so that $\| \hat\Gamma_D - \Gamma \|_F = O_P(1/\sqrt n)$ using Prokhorov's theorem.

It is now enough to prove convergence in probability of the individual eigenvalue estimates $\hat\lambda_i; i = 1,...,p$. For this, define estimates $\tilde\lambda_i$ as median-of-small-variances estimator of the \textit{true} score vectors $\Gamma^T X$. For this we have
%
\begin{equation}\label{eqn:PluginSigmaProof1}
| \tilde\lambda_i - \lambda_i | \stackrel{P}{\rightarrow} 0
\end{equation}
%
using Theorem 3.1 of \cite{Minsker15}, with $\mu = \lambda_i$. Now $ \hat\lambda_i = \text{med}_j (Var( X_{G_j}^T \hat\bfgamma_{D,i} ))$ and $\tilde\lambda_i = \text{med}_j (Var( X_{G_j}^T \bfgamma_i )) $, so that
%
\begin{eqnarray*}
| \hat\lambda_i - \tilde\lambda_i | &\leq & \text{med}_j \left[ Var( X_{G_j}^T ( \hat\bfgamma_{D,i} - \bfgamma_i) ) \right]
\\ &\leq & \| \hat\bfgamma_{D,i} - \bfgamma_i \|^2 \text{med}_j  \left[ \text{Tr} (Cov ( X_{G_j})) \right]
\end{eqnarray*}
%
using Cauchy-Schwarz inequality. Combining the facts $ \| \hat\bfgamma_{D,i} - \bfgamma_i \| = O_P(1/\sqrt n)$ and $ \text{med}_j  [ \text{Tr} (Cov ( X_{G_j})) ] \stackrel{P}{\rightarrow} \text{Tr}(\Sigma)$ \citep{Minsker15} with (\ref{eqn:PluginSigmaProof1}), we get the needed.

\end{proof}