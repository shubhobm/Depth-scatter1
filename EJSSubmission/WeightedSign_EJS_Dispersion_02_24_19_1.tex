\section{A Robust Measure of  Dispersion} 
\label{Sec:WSDispersion1}

From this section onwards, we assume that $\cX = \BR^{p}$, that is, the support of the 
random variable under study is the $p$-dimensional Euclidean plane, and the data 
$X_{1}, \ldots, X_{n}$ are independent and identically distributed from an 
elliptical distribution $\BF$ with parameters $\mu$ and $\Sigma$.  We also 
assume that $X_{1}$ is absolutely continuous, with $\BP [ | X_{1}| = 0] = 0$, 
and that $\Sigma$ is positive definite. This  eliminates technicalities arising from
 rank deficient cases, and makes the weight functions  affine invariant.
Thus, we essentially restrict the rest of this paper to the same  framework as in 
Corollary~\ref{Cor:Elliptic_ARE}. Most of the results below generalize to the case where 
$\cX$ is a separable Hilbert space, however additional technicalities are involved, as in 
\cite{ref:AoS112852_Balietal_PCA_Functional_Robust}, and will be considered 
in a future project. 


Consider the spectral decomposition of $\Sigma$ given by
 $\Sigma = \Gamma\Lambda\Gamma^T$, where $\Gamma$ is an orthogonal matrix 
 and $\Lambda$ is diagonal with positive diagonal elements $\lambda_1 \geq \ldots \geq 
 \lambda_p$.
Also denote the $i$-th eigenvector of $\Sigma$ by 
$\bfgamma_{i} = (\gamma_{i, 1}, \ldots, \gamma_{i, p})^T$ for $1 \leq i \leq p$. Thus, the 
$i$-th column of $\Gamma$ is $\bfgamma_{i}$.
  In the rest of this paper we use the notation 
 $\Sigma^{-1/2} = \Lambda^{-1/2} \Gamma^{T}$, and 
 hence $Z = \Lambda^{-1/2} \Gamma^T (X - \mu)$. 
Recall from Section~\ref{Sec:WSQuantiles} that we use the notation $\BF_{Z}$ for 
the distribution of $Z$, and that $\BF_{Z}$ is a spherically symmetric distribution 
and hence depends only on $|z|$.
Additionally, to simplify notations, for any random variable $X \sim \BF$, we occasionally 
use the abbreviated notation $W (X) \equiv W (X, \BF)$. Note that $W (X)$ is a 
\textit{random weight}, and takes the same value as $W (Z, \BF_{Z})$.  
Also, as noted in  Corollary~\ref{Cor:Elliptic_ARE}, 
 $W (Z)$ is a function of $|Z|$ only. 
 We additionally assume that $\BE W^{2} (X) < \infty$.
 
It is convenient to write 
\ban
X = \mu + R \Gamma \Lambda^{1/2}  U
\ean
where $U$  is a random variable uniformly distributed on the unit sphere 
${\cS}_{0; 1} = \{ x \in \cX: | x | = 1 \}$ and $R$ is another random variable independent 
of $U$ satisfying $\BE R^{2} = p$. Note that  $Z = R U$, and $|Z| = R$, $Z/|Z| = U$. 
Then we have
\ban 
S (X; \mu) & = |X - \mu|^{-1} (X - \mu) 
%\\
%& 
= | \Lambda^{1/2} R U|^{-1} R \Gamma  \Lambda^{1/2}  U\\
& =  | \Lambda^{1/2} U|^{-1} \Gamma \Lambda^{1/2}  U 
%\\
%& 
=  | \Lambda^{1/2} Z|^{-1} \Gamma \Lambda^{1/2}  Z
\ean


The rest of this paper is built on the following transformed random variable:
\baq
\tilde{X} = W (X, \BF) S(X; \mu) \equiv W (Z, \BF_{Z})  
| \Lambda^{1/2} Z|^{-1} \Gamma \Lambda^{1/2}  Z. 
\label{eq:tildeX}
\eaq
We define the notation $\BS (X; \mu) =  S (X; \mu) S( X; \mu)^T$.
Then, we define the following dispersion  parameter:
\baq
\tilde{\Sigma} = \BE \tilde{X} \tilde{X}^{T}
=  \BE W^{2} (X, \BF) \BS (X; \mu). 
\label{eq:tildeSigma}
\eaq
 In the following Theorem, we establish that the eigenvectors of ${\Sigma}$ and 
 $\tilde{\Sigma}$ are identical, although their eigenvalues may be different.
  
\begin{Theorem}
\label{Thm:WSVariance}
Under the conditions listed above, we have
$\tilde{\Sigma} = \Gamma \tilde{\Lambda} \Gamma^{T}$, where 
$\tilde{\Lambda} = \Lambda^{1/2} \BE W^{2} (X) 
	\BE UU^{T}/(U^{T}\Lambda U) \Lambda^{1/2}$
is a diagonal matrix.  Thus, the eigenvectors of ${\Sigma}$ and 
 $\tilde{\Sigma}$ are identical.
\end{Theorem}

\begin{proof}[Proof of Theorem~\ref{Thm:WSVariance}]

Fix any index $i \in \{1, \ldots, p \}$. Consider the vector $\tilde{U}$ such that 
\ban 
\tilde{U}_{j} = \left\{ \begin{array}{ll}
U_{j} & \text{ if } j \ne i, \\
- U_{i} & \text{ if } j = i. 
\end{array}
\right. 
\ean
Then $\tilde{U}$ and $U$ have the same distribution, and note that 
$U^{T} \Lambda U = \tilde{U}^{T} \Lambda \tilde{U}$ almost surely. Consequently, 
for any $j \ne i$ we have
\ban
\BE {\frac{U_{i} U_{j}}{U^{T} \Lambda U}}  
& = \BE {\frac{\tilde{U}_{i} \tilde{U}_{j}}{\tilde{U}^{T} \Lambda \tilde{U}}} 
%\\
%& 
= - \BE {\frac{{U}_{i} {U}_{j}}{U^{T} \Lambda U}}.
\ean
Consequently, 
$\BE S (X; \mu) S(X; \mu)^{T} = \Gamma \Lambda_{S} \Gamma^{T}$, 
as established in Theorem~1 of \citet{ref:SPL12765_Taskinenetal}.

Also, since the weight $W(X)$ is a function of $|Z| = R$, we have that $W(X)$ is 
independent of $S (X; \mu)$. Consequently, we have  
  \ban
  \tilde{\Sigma} & = \BE \tilde{X} \tilde{X}^{T} 
%  \\
%&
 = \BE W^{2} (X) S (X; \mu) S(X; \mu)^{T} \\
& =  \BE W^{2} (X)  \BE S (X; \mu) S(X; \mu)^{T} 
%\\
%& 
= \Gamma \Lambda_{W} \Gamma^{T}, 
\ean
where $\Lambda_{W}$ is a diagonal matrix.

\end{proof}


We now discuss the properties of the sample version $\widehat{\tilde{\Sigma}}$ 
computed from $\bfX$. 
In practice, we cannot obtain $W (x) \equiv W (x, \BF)$, 
and consequently use  $W (x, \BF_{n})$ instead. We assume the following conditions:
\begin{enumerate}
\item \textbf{Bounded weights:} The weights $W (\cdot, \cdot)$ are bounded functions. 

\item \textbf{Uniform convergence:} 
\ban 
\sup_{x \in \cX} | W (x, \BF_{n}) - W (x, \BF) | \rightarrow 0 
\ean 
almost surely as $n \rightarrow \infty $.

\item \textbf{Smoothness under perturbation:}
For all $\BF \in \cF$, there exists a $\delta > 0$, possibly depending on $\BF$, such that 
for any $\epsilon \in (0, \delta)$ 
\ban 
\sup_{x \in \cX} \Bigl| W \bigl( x, \BF \bigr) - 
W \bigl( x, (1 - \epsilon)\BF + \epsilon \delta_{x} \bigr) \Bigr| \leq 
\epsilon.
\ean 
 \end{enumerate}
 In the above, $\delta_{x}$ denotes point mass at $x$. 
These property are easily satisfied under for  weight functions  derived 
from standard depth functions, for example, $W_{HSD} (\cdot)$, 
$W_{MhD} (\cdot)$ and $W_{PD} (\cdot)$ discussed earlier.



The following result allows us to use the empirical, plug-in weights and an 
estimated location parameter in the 
weighted dispersion estimator. A natural choice for the location parameter estimator 
is the solution to $\sum_{i = 1}^{n} \tilde{X}_{i} = 0$. 

\begin{Lemma} \label{Lemma:lemma1}
Assume that $\BE \| X - \mu \|^{-4} < \infty$. Also assume that 
we have a location estimator  $\hat{\mu}_n$ satisfying 
$\BE \|\hat{\mu}_n  - \mu \|^{4} = O (n^{-2}) $. Then
\ban
\frac{1}{n} \sum_{i=1}^{n} W_{n}^{2} (X_{i}, \BF_{n}) \BS (X_{i}; \hat{\mu}_{n})  
= \frac{1}{n} \sum_{i=1}^{n} W^{2} (X_{i}, \BF) \BS (X_{i}; {\mu})
+ R_n,
\ean
%
where for any $c \in \BR^{p}$ such that $| c | = $, we have 
$\BE  c^{T} R_{n} c = o (n^{-1})$.
\end{Lemma}

\begin{proof}[Proof of Lemma~\ref{Lemma:lemma1}]
This proof is mostly algebra, and we provide a sketch of the main arguments.
We have
\ban 
& \frac{1}{n} \sum_{i=1}^{n} W_{n}^{2} (X_{i}, \BF_{n}) \BS (X_{i}; \hat{\mu}_{n}) \\
= & \frac{1}{n} \sum_{i=1}^{n} W^{2} (X_{i}, \BF) \BS (X_{i}; {\mu})
+ \frac{1}{n} \sum_{i=1}^{n}  
\bigl\{ W_{n}^{2} (X_{i}, \BF_{n})  -   W^{2} (X_{i}, \BF) \bigr\} \BS (X_{i}; {\mu}) \\
& + \frac{1}{n} \sum_{i=1}^{n} W^{2} (X_{i}, \BF) \bigl\{ 
\BS (X_{i}; \hat{\mu}_{n}) - \BS (X_{i}; {\mu}) \bigr\} \\
& + \frac{1}{n} \sum_{i=1}^{n}  
\bigl\{ W_{n}^{2} (X_{i}, \BF_{n})  -   W^{2} (X_{i}, \BF) \bigr\} 
\bigl\{ \BS (X_{i}; \hat{\mu}_{n}) - \BS (X_{i}; {\mu}) \bigr\}\\
& = \frac{1}{n} \sum_{i=1}^{n} W^{2} (X_{i}, \BF) \BS (X_{i}; {\mu})
+ T_{2} + T_{3} + T_{4}. 
\ean

Using the stated technical conditions, we can now show that 
$\BE  c^{T} T_{j} c = o (n^{-1})$ for $j = 2, 3, 4$. For illutration, we present
the case for $T_{2}$ below. 

Notice that the $(j, k)$-th element of $T_{2}$ is given by 
\ban 
n^{-1} \sum_{i=1}^{n}  | X_{i} - \mu |^{-2}
\bigl\{ W_{n}^{2} (X_{i}, \BF_{n})  -   W^{2} (X_{i}, \BF) \bigr\} 
(X_{i, j} - \mu_{j}) (X_{i, k} - \mu_{k}), 
\ean
and hence 
\ban 
& c^{T} T_{2} c  = \sum_{j, k} c_{j} c_{k}T_{2, j, k} \\
& = n^{-1} \sum_{i=1}^{n}  | X_{i} - \mu |^{-2}
\bigl\{ W_{n}^{2} (X_{i}, \BF_{n})  -   W^{2} (X_{i}, \BF) \bigr\} 
\bigl( \sum_{j} c_{j} (X_{i, j} - \mu_{j}) \bigr)^{2} \\
& \leq 
M n^{-1} \sum_{i=1}^{n}  | X_{i} - \mu |^{-2}
\bigl\{ | W_{n} (X_{i}, \BF_{n})  -   W (X_{i}, \BF) | \bigr\} 
\bigl( c^{T} (X_{i} - \mu) \bigr)^{2} \\
& \leq 
M n^{-1} \sum_{i=1}^{n}  | X_{i} - \mu |^{-2}
\bigl\{ | W_{n} (X_{i}, \BF_{n})  -   W_{n} (X_{i}, \BF_{n, -i})  | \bigr\} 
\bigl( c^{T} (X_{i} - \mu) \bigr)^{2} 
\\ & \hspace{1cm}
+ M n^{-1} \sum_{i=1}^{n}  | X_{i} - \mu |^{-2}
\bigl\{ | W_{n} (X_{i}, \BF_{n, -i})  -   W (X_{i}, \BF) | \bigr\} 
\bigl( c^{T} (X_{i} - \mu) \bigr)^{2} 
\\
& =  M n^{-1} \sum_{i=1}^{n}  T_{2 1 i}  + M n^{-1} \sum_{i=1}^{n}   T_{2 2 i} \\
& = T_{21} + T_{22}.
\ean
Let $H (X_{i}) = | X_{i} - \mu |^{-2} \bigl( c^{T} (X_{i} - \mu) \bigr)^{2}$, and notice 
that $H (X_{i}) \leq 1$ almost surely for $|c| = 1$.  
Now notice that conditional on $X_{i}$ except for a null set $A_{i}$ (possibly depending 
on $X_{i}$) we have  
$T_{2 1 i} \leq n^{-1}  H (X_{i}) $. Thus, except for a null set 
$A_{1} \cap \ldots \cap A_{n}$, $T_{21} \leq M n^{-2}  H (X_{i})$ and the conclusion 
follows for this part.

The argument for $T_{22}$ follows a similar argument.
\end{proof}




Let $vec~(\BS (X; \mu))$ be the vectorized version of  $\BS (X; \mu)$.
We are now in a position to state the result for consistency of the sample 
version of $\tilde{\Sigma}$,

\begin{Theorem} \label{Theorem:CLT1}
Assume the conditions of Lemma~\ref{Lemma:lemma1}. Then
\ban
& n^{1/2} \sum_{i = 1}^{n} \Bigl( 
W_{n}^{2} (X_{i}, \BF_{n}) ~vec~(\BS (X_{i}; \hat{\mu}_{n})) 
- \BE W^{2} (X_{i}) ~vec~(\BS (X_{i}; \mu)) \Bigr)\\
& \hspace{0.5cm} \draro
N_{p^2} \bigl( 0, V_{W} \bigr),
\ean
where $V_{W} = \BV [W^{2} (X, \BF) ~vec~(\BS (X; {\mu})) ] $.
\end{Theorem}

The asymptotic normality  follows from our assumptions and as a direct consequence 
of Lemma~\ref{Lemma:lemma1}. Incidentally, an expression for $V_{W}$
can be explicitly obtained in terms of $\Gamma$, $\Lambda$ and $\BF$, but is 
algebraic in nature and hence omitted here. 



We now use Theorem~\ref{Theorem:CLT1} to obtain consistency results for 
the eigenvectors obtained from $\widehat{\tilde{\Sigma}} = n^{-1} 
\sum_{i=1}^{n} W^{2} (X_{i}, \BF) \BS (X_{i}; \hat{\mu}_{n})$. 
Suppose that  $\tilde{\Lambda}_{1} > \tilde{\Lambda}_{2} > \ldots > \tilde{\Lambda}_{p}$
are the eigenvalues of $\tilde{\Sigma}$, which we assume are all distinct values. 

\begin{Theorem} \label{Theorem:Eigen1}
Suppose the  spectral decomposition of $\widehat{\tilde{\Sigma}}$ is given 
by $\widehat{\tilde{\Sigma}} = \widehat{\Gamma} \widehat{\tilde{\Lambda}}
\widehat{\Gamma}^T $. Then the matrix of centered and scaled eigenvectors 
$G_{n} = n^{1/2} (\widehat{\Gamma} - \Gamma) $ 
and the vector of centered and scaled eigenvalues 
$L_{n} = n^{1/2} (\widehat{\tilde{\Lambda}} - {\tilde{\Lambda}}) $ have 
independent distributions. The distribution of the random variable $vec~(G_{n})$ converges 
weakly to a $p^2$-variate normal distribution with mean ${\bf 0}_{p^2}$ and the
variance matrix whose $(i, j)$-th block of $p \times p$ entries are given by
\baq
& \sum_{k=1, {}_{k \neq i}}^{p} 
\Bigl[ \tilde{\Lambda}_{i} -  \tilde{\Lambda}_{k} \Bigr]^{-2}
\BE \Bigl[ 
W^4 (Z, \BF_{Z}) \big( 
\BS_{i, k} ({\Lambda}^{1/2} Z; {\bf 0})
\big)^2 
\Bigr]
\bfgamma_k \bfgamma_k^T, 
\text{ if } i = j, \label{equation:DevEq} \\
& - 
\Bigl[ \tilde{\Lambda}_{i} -  \tilde{\Lambda}_{j} \Bigr]^{-2}
\BE \Bigl[ 
W^4 (Z, \BF_{Z}) \big( 
\BS_{i, j} ({\Lambda}^{1/2} Z; {\bf 0})
\big)^2 
\Bigr]
\bfgamma_i \bfgamma_j^T; \text{ if } i \neq j.
\eaq
The distribution of  $L_{n}$ converges 
weakly to a $p$-dimensional normal distribution with mean ${\bf 0}_{p}$ and the
variance-covariance matrix whose $(i, j)$-the element is
\ban
& \BE \Bigl[ 
W^4 (Z, \BF_{Z}) \big( 
\BS_{i, i} ({\Lambda}^{1/2} Z; {\bf 0})
\big)^2 
\Bigr]
- \tilde{\Lambda}_{i}^2, \text{ if } i = j, \\
& \BE \Bigl[ 
W^4 (Z, \BF_{Z}) \big( 
\BS_{i, j} ({\Lambda}^{1/2} Z; {\bf 0})
\big)^2 
\Bigr]
- \tilde{\Lambda}_{i} \tilde{\Lambda}_{j}, \text{ if } i \neq j.
\ean
\end{Theorem}


The proof of this result follows from using Theorem~\ref{Theorem:CLT1} and using 
techniques similar to a corresponding result in \citet{ref:SPL12765_Taskinenetal}, 
and we omit the  algebraic details here.

Recall that the asymptotic variance of the $i$-th eigenvector of the 
\textit{sample covariance matrix}, say $\hat{\bfgamma}_i$ is \citep{ref:AndersonBook09}:
%
\begin{equation} \label{equation:covevEq}
A\BV( \sqrt n\hat \bfgamma_{i}) = 
\sum_{k=1; k \neq i}^p 
\frac{\lambda_i \lambda_k}{(\lambda_i - \lambda_k)^2} \bfgamma_k \bfgamma_k^T; 
\quad 1 \leq i \leq p.
\end{equation}
%
Suppose $\widehat{\tilde{\bfgamma}}_i$ is the $i$-th eigenvector of 
$\widehat{\tilde{\Sigma}}$, whose asymptotic behavior is presented above in 
Theorem~\ref{Theorem:Eigen1}.

This leads to the following useful result:
\begin{Corollary}
The asymptotic relative efficiency of $\widehat{\tilde{\bfgamma}}_i$, 
relative to $\hat{\bfgamma}_i$, is given by 
\ban
& ARE (\widehat{\tilde{\bfgamma}}_i, \hat{\bfgamma}_i; \BF) \\
& =  
\Bigl[ \sum_{k=1; k \neq i}^p \frac{\lambda_i \lambda_k}{(\lambda_i - \lambda_k)^2} \Bigr]
 \Bigl[
\sum_{k=1, {}_{k \neq i}}^{p} 
\biggl[ \tilde{\Lambda}_{i} -  \tilde{\Lambda}_{k} \biggr]^{-2}
\BE \biggl[ 
W^4 (Z, \BF_{Z}) \big( 
\BS_{i, k} ({\Lambda}^{1/2} Z; {\bf 0})
\big)^2 
\biggr]  
\Bigr]^{-1}.
\ean
\end{Corollary}

The proof of this Corollary is immediate.


\section{An Affine Equivariant Robust Measure of Dispersion}
\label{Sec:WSDispersion2}


A desirable invariance property of any dispersion parameter $T_{X}$ corresponding to 
a random variable $X$ is that under affine transformation $Y = AX + b$ the dispersion 
parameter scales to $T_{Y} = A T_{X} A^{T}$. It is clear that $\tilde{\Sigma}$ does 
not possess this property, since it remains unchanged for $X$ and $Y = c X$ for any 
scalar $c > 0$. 

We follow the general framework of M-estimation with data-dependent weights 
\citep{ref:HuberBook81} to construct an affine equivariant counterpart of the 
$\tilde{\Sigma}$. 
Specifically, we implicitly define
\begin{equation} \label{eqn:ADCM}
\Sigma_{*} = \frac{p}{ \BV W (X) } 
\BE \left[ \frac{W^{2}(X) (X - \mu) (X - \mu)^T}
{(X - \mu)^T \Sigma_{*}^{-1}(X - \mu)} \right].
\end{equation}
%

To ensure existence and uniqueness of $\Sigma_{*}$, consider the class of 
dispersion parameters $\Sigma_M$ that are obtained as solutions of the following equation:
%
\begin{equation}
\BE \left[ u( | Z_{M} | )  \frac{Z_{M} Z_{M}^T}{| Z_{M} |^2}  - v( | Z_{M} | ) \BI_p \right] = 0
\end{equation}
%
with $Z_M = \Sigma_M^{-1/2} (X - \mu)$. Under the following assumptions on the scalar valued functions $u$ and $v$, the above equation produces a unique solution \citep{ref:HuberBook81}:
%

\vspace{1em}
\noindent\textbf{(C1)} The function $u(r)/r^2$ is monotone decreasing, 
and $u(r)>0$ for $r > 0$;

\noindent\textbf{(C2)}  The function $v(r)$ is monotone decreasing, and 
$v(r) > 0$ for $r > 0$;

\noindent\textbf{(C3)} Both $u(r)$ and $v(r)$ are bounded and continuous;

\noindent\textbf{(C4)} $u(0) / v(0) < p$;

\noindent\textbf{(C5)} For any hyperplane in the sample space $\mathcal X$, 
(i) $P(H) = \BE \{ \cI_{\{X \in H \}} \} < 1 - p v(\infty) / u(\infty)$ and 
(ii) $P(H) \leq 1/p$.
%

\vspace{1em}
\noindent Putting things into context, in our case we have 
$v (\cdot) = p^{-1}\BV W (X)$, 
$u (\cdot) = W^{2}(X)$. 
%We use the notation $\BF_{|Z|}$ for the cumulative distribution function of $|Z|$.
We proceed to verify the other conditions for the weight functions 
$W_{HSD} (\cdot)$, 
$W_{MhD} (\cdot)$ and $W_{PD} (\cdot)$ discussed earlier.

It is easy to verify that the resulting $u (\cdot)$ from the above choices 
satisfy (C1) and (C3). 
Note that $v (\cdot)$ is a finite positive constant, 
and (C2) and (C3) are also easily satisfied. 
Since $u (0) = 0$ in all the above cases, (C4) is also easy to check. Since $X$ is 
absolutely continuous, (C5) holds trivially.



In order to compute the sample version of $\Sigma_{*}$, we solve (\ref{eqn:ADCM}) 
iteratively by obtaining a sequence of positive definite matrices 
$\hat{\Sigma}^{(k)}_{*}$ until convergence. Thus, using the location 
estimator $\hat{\mu}_{n}$, we may iterate
%
\ban
\hat{\Sigma}^{(k+1)}_{*}  = 
\frac{p}{ \BV W (X) } 
\BE \left[ \frac{W^{2}(X) (X - \hat{\mu}_{n}) (X - \hat{\mu}_{n})^T}
{(X - \hat{\mu}_{n})^T (\hat{\Sigma}^{(k)}_{*})^{-1}(X - \hat{\mu}_{n})} \right].
\ean
%

The asymptotic properties of $\hat{\Sigma}_{*}$ can be obtained using methods similar 
to those of Section~\ref{Sec:WSDispersion1}, and techniques presented in 
\cite{ref:Biometrika00603_CrouxHaesbroeck} and elsewhere. We state the following result and omit its proof.

\begin{Theorem}
\label{Thm:Eigen2}
The asymptotic covariance matrix of an eigenvector of the sample 
affine equivariant scatter functional $\hat{\Sigma}_{*}$ is given by
\ban 
V_{12}
\sum_{k=1, k \neq i}^p \frac{\lambda_i \lambda_k}{\lambda_i - \lambda_k} 
\bfgamma_i \bfgamma_k^T,
\ean
where $V_{12}$  is the asymptotic variance of an off-diagonal element of 
$\hat{\Sigma}_{*}$ when the underlying distribution is $\BF_{Z}$. 
It follows that if $\hat{\bfgamma}_{*, i}$ is the $i$-th eigenvector of $\hat{\Sigma}_{*}$,
%
\begin{equation}
ARE (\hat{\bfgamma}_{*, i}, \hat\bfgamma_{i}; \BF) = V_{12}^{-1} = 
\frac{\left[ \BE ( p u (| Z |)  + u'( | Z |) | Z | ) \right]^2}
{p^2 (p+2)^2 \BE (u (| Z |)^2 \BE (\BS_{12} (Z; {\bf 0}))^2}.
\end{equation}
%
\end{Theorem}


\section{Robust Estimation of Eigenvalues, and a Plug-in Estimator of $\Sigma$}
\label{Sec:Eigen}


As seen in Theorem~\ref{Thm:WSVariance}, eigenvalues of the $\tilde{\Sigma}$ 
are not same as the population eigenvalues. In this section, we discuss on robust 
estimation of $\lambda_{i}$'s using $\tilde{\Sigma}$. Assume the data 
is centered, the robust estimator from Section~\ref{Sec:WSQuantiles} suffices. 
We start by computing 
the sample version $\widehat{\tilde{\Sigma}}$ and its spectral decomposition:
\ban 
\widehat{\tilde{\Sigma}} = \widehat{{\Gamma}} \widehat{\tilde{\Lambda}}
\widehat{{\Gamma}}^{T}
\ean 
We then use the following steps:

\begin{enumerate}
\item Randomly divide the sample indices $\{1,2, \ldots, n\}$ into $k$ disjoint groups $\{G_1,\ldots, G_k \}$ of size $\lfloor n/k \rfloor$ each.

\item Transform the data matrix: 
$\bfS = \widehat{{\Gamma}}^{T} \bfX$.

\item Calculate coordinate-wise variances for each group of indices $G_j$:
%
\ban
\lambda_{i, j}^{\dagger} = \frac{1}{|G_j|} \sum_{l \in G_j} \bigl( 
S_{l i} - \bar{S}_{G_{j}, i} \bigr)^2; \quad i = 1, \ldots, p; \  j = 1, \ldots, k.
\ean
where $\bar{\bfS}_{G_j} = (\bar{S}_{G_j, 1}, \ldots, \bar{S}_{G_j,p})^T$ is the vector of 
column-wise means of $\bfS_{G_j}$, the submatrix of $\bfS$ with row indices in $G_j$.
%

\item Obtain estimates of eigenvalues by taking coordinate-wise medians of these variances:
%
\ban
{\lambda}^{\dagger}_{i} = \text{median} (\lambda_{i,1}^{\dagger}, 
\ldots , \lambda_{i,k}^{\dagger} ); \quad 
i = 1, \ldots, p.
\ean
%
\end{enumerate}
%







We collect ${\lambda}^{\dagger}_{i}$, $ i =1, \ldots, p$ in the diagonal matrix 
${\Lambda}^{\dagger} = \diag ({\lambda}^{\dagger}_{1}, \ldots, 
{\lambda}^{\dagger}_{p})$.
The number of subgroups used to calculate this median-of-small-variances estimator can 
be determined following \cite{ref:Bernoulli152308_Minsker_Median_Banach}. 
There can be other ways of estimating 
the eigenvalues of $\Sigma$ using $\bfS$ also, we will pursue such methods elsewhere.
We construct a consistent 
plug-in estimator of the population covariance matrix $\Sigma$ as 
\ban 
{\Sigma}^{\dagger}
= \widehat{{\Gamma}} {\Lambda}^{\dagger} \widehat{{\Gamma}}^{T}.
\ean
Let $| A |_{F}$ denote the Frobenius norm of a matrix $A$, in other words,
$| A |_{F} = (\text{trace} A^{T} A)^{1/2}$.
The following result establishes that this is a consistent estimator of $\Sigma$:

\begin{Theorem}\label{Thm:pluginSigma}
Suppose that as $n \rightarrow \infty$, $k \rightarrow \infty$ and 
$n/k \rightarrow \infty$.
Then we have
%
\ban
\| {\Sigma}^{\dagger} - \Sigma \|_F \stackrel{P}{\rightarrow} 0.
\ean
%
\end{Theorem}

\begin{proof}[Proof of Theorem \ref{Thm:pluginSigma}]
This proof has many algebraic steps, and we sketch the main arguments below.
Suppose $\hat{A} = \widehat{{\Gamma}}^{T}  \Sigma \widehat{{\Gamma}}$. 

Owing to the fact that the Frobenius norm is invariant under rotations and 
that $p$ is finite and fixed, it suffices to show that the off-diagonal elements of 
$\hat{A}$ converge in probability to zero, and that the difference between 
the $i$-th diagonal element of $\hat{A}$ and ${\lambda}^{\dagger}_{i}$ converges to 
zero for any $i = 1, \ldots, p$.

Now notice that from Theorem~\ref{Theorem:Eigen1} we have that $\widehat{{\Gamma}} 
= {{\Gamma}} + R_{n1}$, where the $(i, j)$-th element of the remainder 
$R_{n1, i, j}$ satisfies $\BE  R_{n1, i, j}^{2} = O(n^{-1})$. 

We can show, using standard algebra, that 
\ban
\hat{A} = \Lambda + R_{n2}, 
\ean
where the $(i, j)$-th element of the remainder 
$R_{n2, i, j}$ satisfies $\BE  R_{n2, i, j}^{2} = O(n^{-1})$. 
This follows immediately from above, the fact that $p$ is finite and fixed, and all 
elements of $\Lambda$ are constants. This immediately establishes the case for the 
off-diagonal elements. 

For the diagonal elements, notice that since $k \rightarrow \infty$, each 
coordinate-wise variance $\lambda_{i, j}^{\dagger}$ for each group of indices $G_j$ 
is a consistent estimator of $\lambda_{i}$. The result follows.

\end{proof}

